
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://bk073.github.io/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="https://bk073.github.io/theme/pygments/friendly.min.css">


  <link rel="stylesheet" type="text/css" href="https://bk073.github.io/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://bk073.github.io/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://bk073.github.io/theme/font-awesome/css/solid.css">


    <link href="https://bk073.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Bishwa Karki Atom">





<meta name="author" content="Bishwa Karki" />
<meta name="description" content="In this article, I have written summary of optimization algorithms used in Deep Learning" />
<meta name="keywords" content="pelican, markdown">


<meta property="og:site_name" content="Bishwa Karki"/>
<meta property="og:title" content="Optimization Algorithms"/>
<meta property="og:description" content="In this article, I have written summary of optimization algorithms used in Deep Learning"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://bk073.github.io/2020/06/optimization-algorithm/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2020-06-30 06:42:00+05:45"/>
<meta property="article:modified_time" content="2020-06-30 06:42:00+05:45"/>
<meta property="article:author" content="https://bk073.github.io/author/bishwa-karki.html">
<meta property="article:section" content="Deep Learning"/>
<meta property="article:tag" content="pelican"/>
<meta property="article:tag" content="markdown"/>
<meta property="og:image" content="/images/profile.jpg">

  <title>Bishwa Karki &ndash; Optimization Algorithms</title>

</head>
<body class="light-theme">
  <aside>
    <div>
      <a href="https://bk073.github.io">
        <img src="/images/profile.jpg" alt="Bishwa Karki" title="Bishwa Karki">
      </a>

      <h1>
        <a href="https://bk073.github.io">Bishwa Karki</a>
      </h1>

<p>Trying to share what I learned</p>

      <nav>
        <ul class="list">


              <li>
                <a target="_self"
                   href="https://bk073.github.io/about/#about">
                  About
                </a>
              </li>
              <li>
                <a target="_self"
                   href="https://bk073.github.io/contact/#contact">
                  Contact
                </a>
              </li>

        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-github" href="https://github.com/bk073" target="_blank">
              <i class="fab fa-github"></i>
            </a>
          </li>
          <li>
            <a  class="sc-envelope" href="mailto:karkeebishwa1@gmail.com" target="_blank">
              <i class="fas fa-envelope"></i>
            </a>
          </li>
          <li>
            <a  class="sc-linkedin" href="https://np.linkedin.com/in/bishwakarki" target="_blank">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
      </ul>
    </div>

  </aside>
  <main>

    <nav>
      <a href="https://bk073.github.io">Home</a>

      <a href="/archives">Archives</a>
      <a href="/categories">Categories</a>
      <a href="/tags">Tags</a>

      <a href="https://bk073.github.io/feeds/all.atom.xml">Atom</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="optimization-algorithm">Optimization Algorithms</h1>
    <p>
      Posted on Tue 30 June 2020 in <a href="https://bk073.github.io/category/deep-learning.html">Deep Learning</a>

        &#8226; 4 min read
    </p>
  </header>


  <div>
    <p>In this article I wanted to talk about the series of optimization algorithm in a sequential way from simple gradient descent to ADAM optimizer.</p>
<h2>Gradient Descent</h2>
<p>Gradient Descent is an optimization algorithm used to optimize some cost function in a iterative way.
I think math says alot more then text :), so let's directly jump into it's simple mathematics:</p>
<p><img alt="gradient_descent_derivative" src="../images/gradient_descent_derivative.png"></p>
<p>Given the cost function we calculate partial derivative of the cost function with respect to the parameters. After calculating the partial derivative of the cost function we make and update to the parameters so that each time the cost function moves towards minimum.</p>
<p><img alt="gradient_descent" src="../images/gradient_descent.png"></p>
<p>This is the root theme of gradient descent, calculate the partial derivative of the cost function with respect to the parameters and update the parameters with the partial derivatives to get the minimum cost function for all the data.</p>
<h1>Variations of Gradient Descent</h1>
<ol>
<li><strong> Batch Gradient Descent: </strong></li>
</ol>
<p>In batch gradient descent all the data are taken into consideration. So if we have 10000 data and if we run for 5 epochs then the gradient descent loop will run for 5 times and for each time, the partial derivative is calculated and update is performed for all 10000 data.</p>
<p>But wait, what if the data points are large i.e in millions ?</p>
<p>So in that case, this may not work properly because the entire data must have to fit in CPU or GPU and this runs memory error and update is delayed since it has to calculate partial derivative of all the points before making update.</p>
<ol>
<li><strong> Stochastic Gradient Descent: </strong></li>
</ol>
<p>So from the batch gradient descent we knew that it may not work well on large data. So now we divide the data into chunk of batches which will now fit into CPU/GPU.</p>
<p>In this, we will take 1 data at a single time, so this makes an faster update because we will calculate the cost for each data point and perform update.
But wait: suppose for 5 million of data and running for 5 epochs, in 1 epoch we need to run/iterate 5 millions of times and for 5 epochs 5*5 millions, so this might be slow if we have large data and we run for more number of epochs. </p>
<p>Also, since we are considering 1 example at a time the cost will fluctuate over all the data and it may not reach the minimum.</p>
<ol>
<li><strong> Mini-batch Gradient Descent: </strong></li>
</ol>
<p>Batch G.D converges to minimum but for large data it will be harder to learn and Stochastic G.D converges for large data but we are taking one data at a time so this slow downs the computations.
To overcome these problems we have a Mini-batch G.D .</p>
<p>In Mini-batch neither we will use all the data nor we will use 1 data at each time but we will use batch of fixed number of training data. For example if we have 500 data then lets divide it into chunk of 100, so we will get 5 chunk each of size 100. Now in 1 loop of gradient descent we will calculate the partial derivative for 100 data and make an update and so on for next batch.</p>
<p>So in this case for 1 epoch we will iterate 5 times.</p>
<p><img alt="descent" src="../images/descent.png"></p>
<h2>Exponentially Weighted Averages:</h2>
<p>With gradient descent, we have an hyperparameter known as learning rate. When the learning rate is high then the time for optimization is decreased but there is more oscillations in the parameters during the update. While the learning rate is low then the optimization takes more time to compute. So in order to make computation faster we need to keep learning rate high but with it we must handle oscillation problem of the parameters. So to handle this we have exponentially weighted averages.</p>
<p><img alt="exponentially_mov_avg" src="../images/exponentially_wt_avg.png"></p>
<p>Vt = β * (Vt-1) + (1-β)*NewSample  </p>
<p>Here the beta parameter controls how much weight to give to the N values.</p>
<p>Simple average has higher accuracy then weighted average but the computation cost of simple average is more. So exponentially weighted average is highly suited for machine learning optimizations.</p>
<p><strong> Gradient Descent with momentum: </strong></p>
<p>We have seen how exponentially weighted average works, now we apply it to update the parameters which lets us use larger learning rate.</p>
<p>On each mini-batch, we compute derivate(dw, db) of the parameters.</p>
<p>V_dw = β * V_dw + (1-β) * dw<br>
V_db = β * V_db + (1-β) * db  </p>
<p>Update:</p>
<p>w := w - learning_rate * V_dw<br>
b := b - learning_rate * V_db  </p>
<h2>RMS Prop:</h2>
<p>We have gradient descent optimization, we solved it's problems and now we can use high learning rate. But there are many parameters to be learned in machine learning problems and we want the parameters with high partials derivative to be update faster while parameters with small slope to update slower. Saying that, it means we want to control update to parameters individually.</p>
<p>S_dw = β * S_dw + (1-β) * dw<strong>2<br>
S_db = β * S_db + (1-β) * db</strong>2  </p>
<p>Update:</p>
<p>w := w - learning_rate * dw / Sqrt(S_dw)<br>
b := b - learning_rate * dw / Sqrt(S_db)  </p>
<h2>ADAM:</h2>
<p>When S_dw = 0 then RMS prop will blow up. So ADAM uses both momentum and RSM prop.</p>
<p>V_dw = β_1 * V_dw + (1-β_1) * dw<br>
S_dw = β_2 * S_dw + (1-β_2) * dw**2  </p>
<p>Update:</p>
<p>w := w - learning_rate * (V_dw  / Sqrt(S_dw))  </p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://bk073.github.io/tag/pelican.html">pelican</a>
      <a href="https://bk073.github.io/tag/markdown.html">markdown</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Bishwa Karki ",
  "url" : "https://bk073.github.io",
  "image": "/images/profile.jpg",
  "description": ""
}
</script>


</body>
</html>